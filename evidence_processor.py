"""
Evidence Processor
è™•ç†è­‰æ“šæœå°‹ã€éæ¿¾ã€åˆ†æå’Œé©—è­‰
"""
from llm_helpers import call_llm, parse_json_response
from qa_tool import web_search
from temporal_checker import (
    extract_time_from_claim,
    extract_time_from_evidence,
    normalize_time_expression,
    is_temporally_relevant
)
from datetime import datetime
from urllib.parse import urlparse

# å®˜æ–¹ä¾†æºåŸŸååˆ—è¡¨
OFFICIAL_DOMAINS = {
    # æ”¿åºœåŸŸåï¼ˆå„åœ‹ï¼‰
    '.gov',         # ç¾åœ‹æ”¿åºœ
    '.gov.tw',      # å°ç£æ”¿åºœ
    '.gov.uk',      # è‹±åœ‹æ”¿åºœ
    '.go.jp',       # æ—¥æœ¬æ”¿åºœ
    '.gov.cn',      # ä¸­åœ‹æ”¿åºœ
    '.gov.au',      # æ¾³æ´²æ”¿åºœ
    '.gouv.fr',     # æ³•åœ‹æ”¿åºœ
    '.gc.ca',       # åŠ æ‹¿å¤§æ”¿åºœ
    '.gob.mx',      # å¢¨è¥¿å“¥æ”¿åºœ
    '.gob.es',      # è¥¿ç­ç‰™æ”¿åºœ
    '.gov.sg',      # æ–°åŠ å¡æ”¿åºœ
    '.go.kr',       # éŸ“åœ‹æ”¿åºœ
    
    # æ•™è‚²æ©Ÿæ§‹
    '.edu',         # ç¾åœ‹æ•™è‚²æ©Ÿæ§‹
    '.edu.tw',      # å°ç£æ•™è‚²æ©Ÿæ§‹
    '.ac.uk',       # è‹±åœ‹å­¸è¡“æ©Ÿæ§‹
    '.ac.jp',       # æ—¥æœ¬å­¸è¡“æ©Ÿæ§‹
    '.edu.au',      # æ¾³æ´²æ•™è‚²æ©Ÿæ§‹
}

INTERNATIONAL_ORGS = [
    'un.org',           # è¯åˆåœ‹
    'who.int',          # ä¸–ç•Œè¡›ç”Ÿçµ„ç¹”
    'imf.org',          # åœ‹éš›è²¨å¹£åŸºé‡‘
    'worldbank.org',    # ä¸–ç•ŒéŠ€è¡Œ
    'wto.org',          # ä¸–ç•Œè²¿æ˜“çµ„ç¹”
    'oecd.org',         # ç¶“æ¿Ÿåˆä½œæš¨ç™¼å±•çµ„ç¹”
    'unesco.org',       # è¯åˆåœ‹æ•™ç§‘æ–‡çµ„ç¹”
]


def get_source_credibility_tier(url):
    """
    åˆ¤æ–·ä¾†æºå¯ä¿¡åº¦ç­‰ç´š
    
    Args:
        url: è­‰æ“šä¾†æº URL
    
    Returns:
        "official": æ”¿åºœæ©Ÿæ§‹æˆ–åœ‹éš›çµ„ç¹”
        "standard": ä¸€èˆ¬ä¾†æº
    """
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # ç§»é™¤ www. å‰ç¶´
        if domain.startswith('www.'):
            domain = domain[4:]
        
        # æª¢æŸ¥æ”¿åºœåŸŸåå¾Œç¶´
        for suffix in OFFICIAL_DOMAINS:
            if domain.endswith(suffix):
                return "official"
        
        # æª¢æŸ¥åœ‹éš›çµ„ç¹”
        for org in INTERNATIONAL_ORGS:
            if org in domain:
                return "official"
        
        return "standard"
    except:
        return "standard"


def generate_search_query(claim, search_mode='general'):
    """
    å¾claimä¸­æå–æœ€ä½³æœå°‹é—œéµå­—
    
    Args:
        claim: å¾…é©—è­‰çš„ä¸»å¼µ
        search_mode: 'official' (å®˜æ–¹ä¾†æº) æˆ– 'general' (ä¸€èˆ¬æœå°‹)
    
    Returns:
        å„ªåŒ–å¾Œçš„æœå°‹æŸ¥è©¢å­—ä¸²
    """
    system = (
        "Extract the most important keywords for fact-checking this claim.\n"
        "CRITICAL: Keep the query in the SAME LANGUAGE as the claim.\n"
        "- If claim is in Chinese â†’ return Chinese keywords\n"
        "- If claim is in English â†’ return English keywords\n\n"
        "Return ONLY 2-4 key terms that would help find relevant evidence.\n"
        "Focus on:\n"
        "- Names of people, organizations, places\n"
        "- Specific events or policies\n"
        "- Dates or time periods\n"
        "- Core factual assertions\n\n"
        "CRITICAL for location keywords:\n"
        "- Keep COMPLETE place names with country/region prefix (e.g., 'æ—¥æœ¬æ±åŒ—' not just 'æ±åŒ—', 'Taiwan Taipei' not just 'Taipei')\n"
        "- NEVER drop the country/region name before a location\n"
        "- Examples: 'æ—¥æœ¬å²©æ‰‹ç¸£' âœ“, 'å²©æ‰‹ç¸£' âœ— | 'Japan Iwate' âœ“, 'Iwate' âœ—\n\n"
        "Remove: opinions, adjectives, unnecessary words.\n"
        "Return as a simple search query string (not JSON)."
    )
    
    try:
        query = call_llm(system, f"Claim: {claim}")
        # æ¸…ç†å›æ‡‰ï¼Œç§»é™¤å¼•è™Ÿå’Œå¤šé¤˜ç©ºç™½
        query = query.strip().strip('"').strip("'")
        
        # å¼·åˆ¶åŠ å…¥åœ°åŸŸé—œéµå­—ï¼ˆå¦‚æœclaimä¸­æœ‰ä½†queryä¸­æ²’æœ‰ï¼‰
        location_keywords = ['å°åŒ—', 'è‡ºåŒ—', 'å°ä¸­', 'è‡ºä¸­', 'å°å—', 'è‡ºå—', 'é«˜é›„', 'å°ç£', 'è‡ºç£', 'Taipei', 'Taichung', 'Tainan', 'Kaohsiung', 'Taiwan']
        for loc in location_keywords:
            if loc in claim and loc not in query:
                query = f"{loc} {query}"
                break
        
        # å®˜æ–¹ä¾†æºæ¨¡å¼ï¼šæ·»åŠ  site: éæ¿¾å™¨
        if search_mode == 'official':
            official_sites = [
                "site:.gov", "site:.gov.tw", "site:.gov.uk", "site:.go.jp",
                "site:.edu", "site:.edu.tw", "site:.ac.uk",
                "site:who.int", "site:un.org", "site:imf.org", "site:worldbank.org"
            ]
            site_filter = " OR ".join(official_sites)
            query = f"{query} ({site_filter})"
        
        return query if len(query) > 0 else claim
    except Exception:
        # å‚™ç”¨ï¼šç›´æ¥ä½¿ç”¨claim
        return claim


def is_evidence_potentially_relevant(claim, evidence_title, evidence_body):
    """
    å¿«é€Ÿé éæ¿¾ï¼šåªéæ¿¾æ˜é¡¯éŒ¯èª¤åœ°é»çš„è­‰æ“š
    
    Args:
        claim: å¾…é©—è­‰çš„ä¸»å¼µ
        evidence_title: è­‰æ“šæ¨™é¡Œ
        evidence_body: è­‰æ“šå…§å®¹
    
    Returns:
        True ä¿ç•™ï¼ŒFalse éæ¿¾æ‰
    """
    evidence_text = (evidence_title + " " + evidence_body)
    
    # æª¢æ¸¬claimä¸­çš„å°ç£ç›¸é—œåœ°é»
    taiwan_locations = ['å°åŒ—', 'è‡ºåŒ—', 'å°ä¸­', 'è‡ºä¸­', 'å°å—', 'è‡ºå—', 'é«˜é›„', 'å°ç£', 'è‡ºç£', 'æ–°åŒ—']
    has_taiwan_location = any(loc in claim for loc in taiwan_locations)
    
    # å¦‚æœclaimæåˆ°å°ç£åœ°é»ï¼Œä½†è­‰æ“šæåˆ°æ˜é¡¯ä¸ç›¸é—œçš„åœ‹å¤–åœ°é»ï¼Œå‰‡éæ¿¾
    if has_taiwan_location:
        irrelevant_locations = [
            'San Diego', 'Beijing', 'åŒ—äº¬', 'Shanghai', 'ä¸Šæµ·', 
            'Hong Kong', 'é¦™æ¸¯', 'Tokyo', 'æ±äº¬', 'Seoul', 'é¦–çˆ¾',
            'Singapore', 'æ–°åŠ å¡', 'London', 'New York', 'Paris'
        ]
        # æª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯è¡çªçš„åœ°é»ï¼ˆåŒæ™‚å‡ºç¾åœ¨è­‰æ“šä¸­ä½†ä¸åœ¨claimä¸­ï¼‰
        for irrelevant_loc in irrelevant_locations:
            if irrelevant_loc in evidence_text and irrelevant_loc not in claim:
                return False
    
    # é è¨­ï¼šä¿ç•™è­‰æ“šçµ¦LLMåˆ†æï¼ˆå¯¬é¬†ç­–ç•¥ï¼‰
    return True


def analyze_evidence_stance(claim, evidence_title, evidence_body):
    """
    åˆ¤æ–·å–®å€‹è­‰æ“šèˆ‡claimçš„é—œä¿‚ï¼šæ”¯æŒ/åé§/ç„¡é—œ
    
    Args:
        claim: å¾…é©—è­‰çš„ä¸»å¼µ
        evidence_title: è­‰æ“šæ¨™é¡Œ
        evidence_body: è­‰æ“šå…§å®¹
    
    Returns:
        "support" | "refute" | "irrelevant"
    """
    system = (
        "Analyze if the evidence supports, refutes, or is irrelevant to the claim.\n"
        "Return ONLY one word: support / refute / irrelevant\n"
        "Do not explain, just return the single word."
    )
    
    user = f"Claim: {claim}\n\nEvidence:\nTitle: {evidence_title}\nContent: {evidence_body}"
    
    try:
        result = call_llm(system, user).strip().lower()
        # æ¨™æº–åŒ–å›æ‡‰
        if "support" in result:
            return "support"
        elif "refute" in result or "contradict" in result:
            return "refute"
        else:
            return "irrelevant"
    except Exception:
        return "irrelevant"


def verify_claim(claim, language="zh-TW", temporal_check=True, claim_reference_date=None):
    """
    é©—è­‰å–®å€‹ä¸»å¼µ
    
    Args:
        claim: å¾…é©—è­‰çš„ä¸»å¼µ
        language: å›æ‡‰èªè¨€
        temporal_check: æ˜¯å¦é€²è¡Œæ™‚é–“ç›¸é—œæ€§æª¢æŸ¥ï¼ˆé è¨­é–‹å•Ÿï¼‰
        claim_reference_date: claim çš„ç™¼å¸ƒæ—¥æœŸï¼ˆç”¨æ–¼æ™‚é–“æª¢æŸ¥ï¼‰ï¼ŒNone å‰‡ä½¿ç”¨ä»Šå¤©
    
    Returns:
        {
            "verdict": "Supported" | "Contradicted" | "Insufficient evidence" | "Temporal mismatch",
            "explanation": str,
            "evidence_count": int,
            "search_query": str,
            "evidence_breakdown": {"support": int, "refute": int, "irrelevant": int},
            "temporal_warning": str (optional),
            "source_type": "official" | "general" (optional),
            "authoritative_override": bool (optional)
        }
    """
    # åˆå§‹åŒ–é è¨­å€¼ï¼Œé¿å…è®Šæ•¸æœªå®šç¾©
    search_query = claim
    valid_results = []
    
    # æå– claim ä¸­çš„æ™‚é–“è³‡è¨Šï¼ˆå¦‚æœå•Ÿç”¨æ™‚é–“æª¢æŸ¥ï¼‰
    claim_time_expression = None
    claim_time_info = None
    temporal_warnings = []
    
    if temporal_check:
        try:
            claim_time_expression = extract_time_from_claim(claim)
            if claim_time_expression:
                print(f"  -> ç™¼ç¾æ™‚é–“æè¿°: {claim_time_expression}")
                # ä½¿ç”¨ claim çš„ç™¼å¸ƒæ—¥æœŸä½œç‚ºåƒè€ƒé»
                ref_date = claim_reference_date or datetime.now().isoformat()
                claim_time_info = normalize_time_expression(claim_time_expression, ref_date)
                print(f"  -> æ¨™æº–åŒ–æ™‚é–“: {claim_time_info.get('parsed_date')} ({claim_time_info.get('time_type')})")
        except Exception as e:
            print(f"  Warning: Time extraction failed ({e})")

    # === ç¬¬ä¸€éšæ®µï¼šæœå°‹å®˜æ–¹ä¾†æº ===
    print(f"\n  [éšæ®µ1] æœå°‹å®˜æ–¹ä¾†æº...")
    official_query = generate_search_query(claim, search_mode='official')
    print(f"  -> å®˜æ–¹æœå°‹é—œéµå­—: {official_query}")
    
    try:
        official_results = web_search(official_query, max_results=5)
        print(f"  -> æ‰¾åˆ° {len(official_results)} å€‹æœå°‹çµæœ")
        
        # éæ¿¾å‡ºçœŸæ­£çš„å®˜æ–¹ä¾†æº
        verified_official = []
        for result in official_results:
            if get_source_credibility_tier(result.get('href', '')) == 'official':
                verified_official.append(result)
        
        if verified_official:
            print(f"  [å®˜æ–¹ä¾†æº] æ‰¾åˆ° {len(verified_official)} å€‹å®˜æ–¹ä¾†æºï¼Œç›´æ¥æ¡ä¿¡")
            
            # åˆ†æç¬¬ä¸€å€‹å®˜æ–¹ä¾†æºçš„ç«‹å ´
            first_official = verified_official[0]
            stance = analyze_evidence_stance(
                claim,
                first_official.get('title', ''),
                first_official.get('body', '')
            )
            
            official_url = first_official.get('href', 'N/A')
            
            if stance == 'support':
                return {
                    "verdict": "Supported",
                    "explanation": f"ğŸ›ï¸ å®˜æ–¹ä¾†æºè­‰å¯¦ï¼š{official_url}\n\n{first_official.get('body', '')[:300]}...",
                    "evidence_count": len(verified_official),
                    "search_query": official_query,
                    "source_type": "official",
                    "authoritative_override": True,
                    "evidence_breakdown": {
                        "support": len(verified_official),
                        "refute": 0,
                        "irrelevant": 0,
                        "official_sources": verified_official
                    }
                }
            elif stance == 'refute':
                return {
                    "verdict": "Contradicted",
                    "explanation": f"ğŸ›ï¸ å®˜æ–¹ä¾†æºåé§ï¼š{official_url}\n\n{first_official.get('body', '')[:300]}...",
                    "evidence_count": len(verified_official),
                    "search_query": official_query,
                    "source_type": "official",
                    "authoritative_override": True,
                    "evidence_breakdown": {
                        "support": 0,
                        "refute": len(verified_official),
                        "irrelevant": 0,
                        "official_sources": verified_official
                    }
                }
            else:
                print(f"  [å®˜æ–¹ä¾†æº] å®˜æ–¹ä¾†æºä¸ç›¸é—œï¼Œç¹¼çºŒä¸€èˆ¬æœå°‹")
        else:
            print(f"  [å®˜æ–¹ä¾†æº] æœªæ‰¾åˆ°å¯ä¿¡å®˜æ–¹ä¾†æº")
    except Exception as e:
        print(f"  [å®˜æ–¹ä¾†æº] æœå°‹å¤±æ•—: {e}")
    
    # === ç¬¬äºŒéšæ®µï¼šä¸€èˆ¬æœå°‹ ===
    print(f"\n  [éšæ®µ2] é€²è¡Œä¸€èˆ¬æœå°‹...")
    valid_results = []
    
    try:
        # å…ˆç”Ÿæˆæ›´ç²¾æº–çš„æœå°‹æŸ¥è©¢
        search_query = generate_search_query(claim, search_mode='general')
        print(f"  -> æœå°‹é—œéµå­—: {search_query}")
    except Exception as e:
        print(f"  Warning: Search query generation failed ({e}), using original claim")
        search_query = claim
    
    try:
        # æœå°‹è‡³å°‘10å€‹çµæœ
        search_results = web_search(search_query, max_results=10)
        
        # éæ¿¾æœ‰æ•ˆçµæœ
        valid_results = [r for r in search_results if r.get('title') and r.get('body')]
    except Exception as e:
        print(f"  Error: Search failed ({e})")
        return {
            "verdict": "Insufficient evidence",
            "explanation": f"Search error: {str(e)}",
            "evidence_count": 0,
            "search_query": search_query
        }
    
    # å³ä½¿å°‘æ–¼3å€‹ä¹Ÿç¹¼çºŒåˆ†æï¼Œä½†æœƒåœ¨çµæœä¸­è¨»æ˜
    evidence_warning = ""
    if len(valid_results) < 3:
        evidence_warning = f"[Warning] Only found {len(valid_results)} evidence source(s). Recommended: at least 3 sources."
    
    if len(valid_results) == 0:
        return {
            "verdict": "Insufficient evidence",
            "explanation": "No relevant evidence found. Cannot verify this claim.",
            "evidence_count": 0,
            "search_query": search_query,
            "evidence_breakdown": {"support": 0, "refute": 0, "irrelevant": 0}
        }

    # åˆ†ææ¯å€‹è­‰æ“šçš„ç«‹å ´
    print(f"  â†’ Analyzing stance of {len(valid_results)} evidence sources...")
    
    # å…ˆé éæ¿¾æ˜é¡¯ä¸ç›¸é—œçš„çµæœ
    filtered_results = []
    filtered_out = 0
    for r in valid_results:
        if is_evidence_potentially_relevant(claim, r.get('title', ''), r.get('body', '')):
            filtered_results.append(r)
        else:
            filtered_out += 1
    
    if filtered_out > 0:
        print(f"     Pre-filtered {filtered_out} obviously irrelevant sources")
    
    # æ™‚é–“ç›¸é—œæ€§éæ¿¾ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
    if temporal_check and claim_time_info and claim_time_info.get('time_type') != 'no_time_reference':
        temporally_filtered = []
        temporal_filtered_out = 0
        
        for r in filtered_results:
            # å¾è­‰æ“šä¸­æå–æ™‚é–“è¡¨é”å¼å’Œç™¼å¸ƒæ—¥æœŸ
            evidence_time_data = extract_time_from_evidence(r.get('body', ''))
            evidence_time_expr = evidence_time_data.get('time_expression')
            evidence_pub_date = evidence_time_data.get('publish_date')
            
            if evidence_time_expr:
                # æ±ºå®šåƒè€ƒé»ï¼šå„ªå…ˆä½¿ç”¨è­‰æ“šçš„ç™¼å¸ƒæ—¥æœŸï¼Œå¦å‰‡ä½¿ç”¨ä»Šå¤©
                if evidence_pub_date:
                    try:
                        # å˜—è©¦æ¨™æº–åŒ–è­‰æ“šç™¼å¸ƒæ—¥æœŸ
                        normalized_pub_date = normalize_time_expression(evidence_pub_date, datetime.now().isoformat())
                        reference_date = normalized_pub_date.get('parsed_date', datetime.now().isoformat())
                        print(f"     ä½¿ç”¨è­‰æ“šç™¼å¸ƒæ—¥æœŸä½œç‚ºåƒè€ƒ: {reference_date}")
                    except:
                        reference_date = datetime.now().isoformat()
                        print(f"     è­‰æ“šç™¼å¸ƒæ—¥æœŸæ¨™æº–åŒ–å¤±æ•—ï¼Œä½¿ç”¨ä»Šå¤©ä½œç‚ºåƒè€ƒ")
                else:
                    reference_date = datetime.now().isoformat()
                
                # æ¨™æº–åŒ–è­‰æ“šæ™‚é–“ï¼ˆä½¿ç”¨è­‰æ“šç™¼å¸ƒæ—¥æœŸæˆ–ä»Šå¤©ä½œç‚ºåƒè€ƒé»ï¼‰
                evidence_time_info = normalize_time_expression(evidence_time_expr, reference_date)
                
                # æª¢æŸ¥æ™‚é–“ç›¸é—œæ€§ï¼ˆåƒ…æ¨™è¨˜ï¼Œä¸éæ¿¾ï¼‰
                temporal_result = is_temporally_relevant(claim_time_info, evidence_time_info)
                
                r['temporal_status'] = temporal_result['status']
                r['temporal_info'] = temporal_result
                
                # ä¿ç•™æ‰€æœ‰è­‰æ“šï¼Œåªæ¨™è¨˜æ™‚é–“ç‹€æ…‹
                temporally_filtered.append(r)
                
                if not temporal_result['is_relevant']:
                    temporal_warnings.append(
                        f"âš ï¸ è­‰æ“š '{r.get('title', '')[:50]}...' çš„æ™‚é–“ ({temporal_result['evidence_date']}) "
                        f"ä¸ç¬¦åˆ claim çš„æ™‚é–“ç¯„åœ ({temporal_result['expected_range']})ï¼Œä½†ä»ä¿ç•™ä¾›åˆ†æ"
                    )
            else:
                # ç„¡æ³•æå–æ™‚é–“çš„è­‰æ“šä¿ç•™
                r['temporal_status'] = 'no_constraint'
                temporally_filtered.append(r)
        
        # ç§»é™¤ temporal_filtered_out ç›¸é—œé‚è¼¯ï¼ˆä¸å†éæ¿¾è­‰æ“šï¼‰
        filtered_results = temporally_filtered
    
    # === Step 5: åˆ†ææ¯å€‹è­‰æ“š ===    # å¦‚æœéæ¿¾å¾Œæ²’æœ‰çµæœï¼Œè¿”å›è­‰æ“šä¸è¶³
    if len(filtered_results) == 0:
        return {
            "verdict": "Insufficient evidence",
            "explanation": f"All {len(valid_results)} search results were irrelevant to the claim (wrong location/topic).",
            "evidence_count": len(valid_results),
            "search_query": search_query,
            "evidence_breakdown": {"support": 0, "refute": 0, "irrelevant": len(valid_results)}
        }
    
    categorized_evidence = {
        "support": [],
        "refute": [],
        "irrelevant": []
    }
    
    for r in filtered_results:
        title = r.get('title', '')
        body = r.get('body', '')
        stance = analyze_evidence_stance(claim, title, body)
        
        evidence_item = {
            "title": title,
            "snippet": body[:200] + "..." if len(body) > 200 else body,
            "href": r.get('href', '')
        }
        
        # åŠ å…¥æ™‚é–“è³‡è¨Šï¼ˆå¦‚æœæœ‰ï¼‰
        if 'temporal_info' in r:
            evidence_item['temporal_info'] = r['temporal_info']
        
        categorized_evidence[stance].append(evidence_item)
    
    support_count = len(categorized_evidence["support"])
    refute_count = len(categorized_evidence["refute"])
    irrelevant_count = len(categorized_evidence["irrelevant"])
    
    # åŠ ä¸Šè¢«é éæ¿¾æ‰çš„æ•¸é‡
    total_irrelevant = irrelevant_count + filtered_out
    
    print(f"     Support: {support_count}, Refute: {refute_count}, Irrelevant: {total_irrelevant} (pre-filtered: {filtered_out})")
    
    # å»ºç«‹åˆ†é¡å¾Œçš„è­‰æ“šæ‘˜è¦çµ¦LLM
    context = ""
    
    if categorized_evidence["support"]:
        context += "=== Supporting Evidence ===\n"
        for i, ev in enumerate(categorized_evidence["support"], 1):
            context += f"{i}. [{ev['title']}]\n   {ev['snippet']}\n\n"
    
    if categorized_evidence["refute"]:
        context += "=== Refuting Evidence ===\n"
        for i, ev in enumerate(categorized_evidence["refute"], 1):
            context += f"{i}. [{ev['title']}]\n   {ev['snippet']}\n\n"
    
    if categorized_evidence["irrelevant"]:
        context += f"=== Irrelevant Evidence ({total_irrelevant} sources total, {filtered_out} pre-filtered) ===\n(Not shown for brevity)\n\n"

    # æ ¹æ“šèªè¨€è¨­å®šå›æ‡‰èªè¨€
    language_instruction = ""
    if language == "zh-TW":
        language_instruction = "CRITICAL: You MUST respond in Traditional Chinese (ç¹é«”ä¸­æ–‡). All explanations must be in Traditional Chinese."
    elif language == "en":
        language_instruction = "CRITICAL: You MUST respond in English. All explanations must be in English."
    else:
        language_instruction = "CRITICAL: You MUST respond in Traditional Chinese (ç¹é«”ä¸­æ–‡). All explanations must be in Traditional Chinese."

    system = (
        f"{language_instruction}\n\n"
        "You are verifying a factual claim using categorized evidence.\n"
        f"Evidence summary:\n"
        f"- Supporting evidence: {support_count}\n"
        f"- Refuting evidence: {refute_count}\n"
        f"- Irrelevant evidence: {total_irrelevant} (filtered out)\n\n"
        "Based on the categorized evidence, classify the claim as:\n"
        "- Supported: If supporting evidence is strong and refuting evidence is weak/absent\n"
        "- Contradicted: If refuting evidence is strong and supporting evidence is weak/absent\n"
        "- Insufficient evidence: If evidence is too weak, contradictory, or mostly irrelevant\n\n"
        "In your explanation, mention:\n"
        "1. Key supporting/refuting evidence\n"
        "2. Why you reached this conclusion\n"
        "3. Any uncertainty or conflicting information\n\n"
        f"{evidence_warning}\n"
        f"IMPORTANT: Your entire response (verdict + explanation) must be in the language specified above.\n"
        "Return JSON with fields: verdict, explanation."
    )

    user = f"Claim:\n{claim}\n\n{context}"

    out = call_llm(system, user)

    try:
        result = parse_json_response(out)
        result['evidence_count'] = len(valid_results)
        result['search_query'] = search_query
        result['evidence_breakdown'] = {
            "support": support_count,
            "refute": refute_count,
            "irrelevant": total_irrelevant
        }
        if evidence_warning:
            result['explanation'] = evidence_warning + "\n\n" + result['explanation']
        
        # åŠ å…¥æ™‚é–“è­¦å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
        if temporal_warnings:
            result['temporal_warning'] = temporal_warnings[0]  # åªé¡¯ç¤ºç¬¬ä¸€å€‹è­¦å‘Š
        
        return result
    except Exception as e:
        return {
            "verdict": "Insufficient evidence",
            "explanation": f"Unable to parse verification result. {evidence_warning}",
            "evidence_count": len(valid_results),
            "search_query": search_query,
            "evidence_breakdown": {
                "support": support_count,
                "refute": refute_count,
                "irrelevant": total_irrelevant
            }
        }
